{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from GPy.models import GPRegression\n",
    "\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "from emukit.core import ParameterSpace, ContinuousParameter\n",
    "from emukit.experimental_design.model_free.latin_design import LatinDesign\n",
    "from emukit.bayesian_optimization.loops import BayesianOptimizationLoop\n",
    "from emukit.bayesian_optimization.acquisitions import (\n",
    "    NegativeLowerConfidenceBound as LCB,\n",
    "    ExpectedImprovement as EI)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from l2_bayes_opt.acquisitions import (\n",
    "    L2NegativeLowerConfidenceBound as L2_LCB,\n",
    "    L2ExpectedImprovement as L2_EI)\n",
    "from l2_bayes_opt.utils import BayesOptPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\N}{\\mathcal{N}}\n",
    "\\newcommand{\\e}{\\epsilon}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\GP}{\\mathcal{GP}}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "$\n",
    "\n",
    "# Background\n",
    "This notebook demonstrates the method proposed in [Uhrenholt and Jensen, 2019] for target vector estimation using Bayesian optimization. Given a target vector $\\y^* \\in \\R^K$ and multi-output function $h : \\R^M \\rightarrow \\R^K$, which is assumed both noisy and blackboxed, we seek to find an optimal input $\\hat\\x$ such that $\\y^* \\approx h(\\hat\\x)$.\n",
    "\n",
    "A straighforward approach would be to fit a probabilistic model, such as a Gaussian process model, directly to the squared Euclidean distance between target and function output, i.e.:\n",
    "\\begin{align*}\n",
    "d &= \\| h(\\x) - \\y^* \\|^2_2,\n",
    "\\\\\n",
    "d &\\sim \\GP\\left(0, \\kappa(\\x, \\x')\\right),\n",
    "\\end{align*}\n",
    "and then subsequently minimize $d$ by standard Bayesian optimization.\n",
    "\n",
    "In the proposed method we instead fit a separate model to each output dimension of $h$:\n",
    "\\begin{align*}\n",
    "y_k &= h(\\x)_k \\sim \\GP\\left(0, \\kappa_k(\\x, \\x')\\right), \\qquad 1 \\le k \\le K,\n",
    "\\end{align*}\n",
    "yielding a predictive normal distribution for each dimension. Based on the dimensions' joint distribution we can infer an approximate noncentral Chi-squared distribution over $d$ and use this for a more accurate optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example problem definition\n",
    "\n",
    "Define noisy multi-output function\n",
    "\n",
    "\\begin{align*}\n",
    "h(\\mathbf{x}) &= \\mathbf{y} =\n",
    "\\begin{bmatrix}\n",
    "5 + \\sin(x) + \\epsilon_1 \\\\ 2 + 1.3\\cos(x)+ \\epsilon_2  \\\\ \\tanh(x)+ \\epsilon_3\n",
    "\\end{bmatrix}, \\\\\n",
    "\\epsilon_1, \\epsilon_2, \\epsilon_3 &\\sim \\mathcal{N}\\left(0, 10^{-2}\\right),\n",
    "\\end{align*}\n",
    "\n",
    "and associated distance function\n",
    "\n",
    "$$\n",
    "d(\\mathbf{x}) = \\| d(\\mathbf{x}) - \\mathbf{y}^* \\|^2_2,\n",
    "$$\n",
    "\n",
    "for target $\\mathbf{y}^* = [4.5, 2, 0.8]^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_noiseless(x):\n",
    "    f1 = 5 + np.sin(x)\n",
    "    f2 = 2 + 1.3 * np.cos(x)\n",
    "    f3 = np.tanh(x)\n",
    "    return np.hstack((f1, f2, f3))\n",
    "\n",
    "def h(x):\n",
    "    res = h_noiseless(x)\n",
    "    return res + norm.rvs(scale=0.1, size=res.shape)\n",
    "\n",
    "def d_noiseless(x):\n",
    "    return ((h_noiseless(x) - target)**2).sum(axis=1)[:, None]\n",
    "\n",
    "def d(x):\n",
    "    return ((h(x) - target)**2).sum(axis=1)[:, None]\n",
    "\n",
    "target = np.array([4.5, 2.0, 0.8])\n",
    "xmin, xmax = 0, 2*np.pi\n",
    "\n",
    "plotter = BayesOptPlotter(h_noiseless, target, xmin, xmax)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "fig.suptitle(\"Function outputs\")\n",
    "plotter.plot_function(axes)\n",
    "plotter.decorate(axes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "fig.suptitle(\"Distance to target\")\n",
    "plotter.plot_distance(ax)\n",
    "plotter.decorate(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 5 points by latin hypercube sampling and calculate associated function and distance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "parameter_space = ParameterSpace([ContinuousParameter(\"x\", xmin, xmax)])\n",
    "latin_design = LatinDesign(parameter_space=parameter_space)\n",
    "X0 = latin_design.get_samples(n_samples)\n",
    "Y0 = h(X0)\n",
    "D0 = ((Y0 - target)**2).sum(axis=1)\n",
    "plotter = BayesOptPlotter(h_noiseless, target, xmin, xmax, X0=X0, Y0=Y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison\n",
    "\n",
    "### Standard GP model\n",
    "Fit a GP directly to the sampled distances and plot the associated LCB acquisition utility. The utility reflects the prospect of sampling a given point, according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPRegression(X0, D0[:, None])\n",
    "model.optimize_restarts(verbose=False)\n",
    "model_wrapped = GPyModelWrapper(model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "fig.suptitle(\"Function outputs\")\n",
    "plotter.plot_function(axes)\n",
    "plotter.decorate(axes)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.set_title(\"Distance to target\")\n",
    "plotter.plot_distance(ax1)\n",
    "plotter.plot_distance_prediction(ax1, model, multi=False)\n",
    "\n",
    "ax2.set_title(\"Negative Lower Confidence Bound\")\n",
    "acq = LCB(model=model_wrapped)\n",
    "plotter.plot_acquisition(ax2, acq)\n",
    "plotter.decorate([ax1, ax2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed model\n",
    "\n",
    "Fit a multi-output GP to the individual function output dimensions and use the predictive Chi-squared distribution for a more robust LCB acquisition utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPRegression(X0, Y0)\n",
    "model.optimize_restarts(verbose=False)\n",
    "model_wrapped = GPyModelWrapper(model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "fig.suptitle(\"Function outputs\")\n",
    "plotter.plot_function(axes)\n",
    "plotter.plot_function_predictions(axes, model)\n",
    "plotter.decorate(axes)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.set_title(\"Distance to target\")\n",
    "plotter.plot_distance(ax1)\n",
    "plotter.plot_distance_prediction(ax1, model, multi=True)\n",
    "\n",
    "ax2.set_title(\"Negative Lower Confidence Bound\")\n",
    "acq = L2_LCB(model=model_wrapped, target=target)\n",
    "plotter.plot_acquisition(ax2, acq)\n",
    "plotter.decorate([ax1, ax2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization\n",
    "\n",
    "We use the Bayesian optimization framework from emukit [cite] to compare the two approaches. For each model we use the same 5 starting points and run 5 iterations. In each iteration the point to sample is selected by finding the input which maximizes the given acquisition function. The aim is to find the input which yields the lowest distance.\n",
    "\n",
    "### Standard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = GPRegression(X0, D0[:, None])\n",
    "model.optimize_restarts(verbose=False)\n",
    "model_wrapped = GPyModelWrapper(model)\n",
    "acq = LCB(model=model_wrapped)\n",
    "\n",
    "fit_update = lambda a, b: model.optimize_restarts(verbose=False)\n",
    "bayesopt_loop = BayesianOptimizationLoop(\n",
    "    model=model_wrapped, space=parameter_space, acquisition=acq)\n",
    "bayesopt_loop.iteration_end_event.append(fit_update)\n",
    "bayesopt_loop.run_loop(d, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = BayesOptPlotter(h_noiseless, target, xmin, xmax)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "fig.suptitle(\"Function outputs\")\n",
    "plotter.plot_function(axes, plot_samples=False)\n",
    "plotter.decorate(axes)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.set_title(\"Distance to target\")\n",
    "plotter.plot_distance(ax1)\n",
    "plotter.plot_distance_prediction(ax1, model, multi=False)\n",
    "ax1.plot(model.X.ravel(), model.Y, \"ko\")\n",
    "\n",
    "ax2.set_title(\"Negative Lower Confidence Bound\")\n",
    "plotter.plot_acquisition(ax2, acq)\n",
    "plotter.decorate([ax1, ax2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = GPRegression(X0, Y0)\n",
    "model_wrapped = GPyModelWrapper(model)\n",
    "acq = L2_LCB(model=model_wrapped, target=target)\n",
    "\n",
    "fit_update = lambda a, b: model.optimize_restarts(verbose=False)\n",
    "bayesopt_loop = BayesianOptimizationLoop(\n",
    "    model=model_wrapped, space=parameter_space, acquisition=acq)\n",
    "bayesopt_loop.iteration_end_event.append(fit_update)\n",
    "bayesopt_loop.run_loop(h, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "fig.suptitle(\"Function outputs\")\n",
    "plotter.plot_function(axes, plot_samples=False)\n",
    "plotter.plot_function_predictions(axes, model)\n",
    "plotter.decorate(axes)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.set_title(\"Distance to target\")\n",
    "plotter.plot_distance(ax1)\n",
    "plotter.plot_distance_prediction(ax1, model, multi=True)\n",
    "D = ((model.Y - target)**2).sum(axis=1)\n",
    "ax1.plot(model.X.ravel(), D, \"ko\")\n",
    "\n",
    "ax2.set_title(\"Negative Lower Confidence Bound\")\n",
    "plotter.plot_acquisition(ax2, acq)\n",
    "plotter.decorate([ax1, ax2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
